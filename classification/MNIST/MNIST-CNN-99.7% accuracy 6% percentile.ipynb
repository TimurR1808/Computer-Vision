{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b9c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c916ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe789f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_train = pd.read_csv('C:/datasets/digit-recognizer/train.csv').drop('label',axis=1).values\n",
    "kaggle_labels = pd.read_csv('C:/datasets/digit-recognizer/train.csv')['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a2e44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not normalized images (0-255 pixels)\n",
    "kaggle_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fea4b3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73489d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "kaggle_train = scaler.fit_transform(kaggle_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb08ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_train = torch.tensor(kaggle_train, dtype=torch.float32).view(42000,1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfc37066",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_labels = torch.tensor(kaggle_labels, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38fcbd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_train.dtype, kaggle_labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d78c10af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(image_dataset):\n",
    "    if str(image_dataset.dtype)[:5] == 'torch' and len(image_dataset.shape) == 4:\n",
    "        for i in range(image_dataset.shape[0]):\n",
    "            new = image_dataset[i][0].flatten()\n",
    "            new = 2*(new/255)-1\n",
    "            image_dataset[i][0] = new.view(28,28) \n",
    "        return image_dataset\n",
    "    else:\n",
    "        raise Exception(\"IMAGE DATASET DOES NOT MEET REQUIREMENTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b322f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle_train = normalize_dataset(kaggle_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93768c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.x_train = features\n",
    "        self.y_train = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_train[idx],self.y_train[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "307a2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(15), \n",
    "                                       transforms.ToTensor()\n",
    "#                                         transforms.Normalize(0.5,0.5)\n",
    "                                       ]) \n",
    "# transforms.RandomRotation(20),\n",
    "# transforms.CenterCrop(28),\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                    transforms.ToTensor()\n",
    "#                                  transforms.Normalize(0.5,0.5)\n",
    "                               ])\n",
    "\n",
    "kaggle_train = KaggleDataset(kaggle_train, kaggle_labels)\n",
    "\n",
    "train = datasets.MNIST(root='data', train=True,\n",
    "                                   download=True, transform=train_transforms)\n",
    "train_valid = datasets.MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "\n",
    "test = datasets.MNIST(root='data', train=False,\n",
    "                                  download=True, transform=train_transforms)\n",
    "\n",
    "test_valid = datasets.MNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecdebbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "valid_size = 0.060952381\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_kaggle_train = len(kaggle_train)\n",
    "indices = list(range(num_kaggle_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_kaggle_train))\n",
    "kaggle_train_idx, kaggle_train_valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "kaggle_train_sampler = SubsetRandomSampler(kaggle_train_idx)\n",
    "kaggle_train_valid_sampler = SubsetRandomSampler(kaggle_train_valid_idx)\n",
    "\n",
    "kaggle_train_loader = torch.utils.data.DataLoader(kaggle_train, batch_size=64,\n",
    "    sampler=kaggle_train_sampler, num_workers=0)\n",
    "kaggle_train_valid_loader = torch.utils.data.DataLoader(kaggle_train, batch_size=64, \n",
    "    sampler=kaggle_train_valid_sampler, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "423b39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 0.064\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, train_valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "train_valid_sampler = SubsetRandomSampler(train_valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=64,\n",
    "    sampler=train_sampler, num_workers=0)\n",
    "train_valid_loader = torch.utils.data.DataLoader(train_valid, batch_size=64, \n",
    "    sampler=train_valid_sampler, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38fe56ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 0.128\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_test = len(test)\n",
    "indices = list(range(num_test))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_test))\n",
    "test_idx, test_valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "test_valid_sampler = SubsetRandomSampler(test_valid_idx)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=64,\n",
    "    sampler=test_sampler, num_workers=0)\n",
    "test_valid_loader = torch.utils.data.DataLoader(test_valid, batch_size=64, \n",
    "    sampler=test_valid_sampler, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b5bb826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39440, 56160, 8720)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for training\n",
    "len(kaggle_train_loader.sampler), len(train_loader.sampler), len(test_loader.sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "900a489b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2560, 3840, 1280)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for validation\n",
    "len(kaggle_train_valid_loader.sampler), len(train_valid_loader.sampler), len(test_valid_loader.sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9184591",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter1 = iter(kaggle_train_loader)\n",
    "iter2 = iter(train_loader)\n",
    "iter3 = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d373f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1, lbl1 = next(iter1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "746bf68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 9, 3, 9, 4, 3, 0, 1, 6, 6, 2, 0, 1, 0, 0, 7, 1, 2, 8, 7, 9, 4, 0, 1,\n",
       "        1, 4, 1, 0, 2, 0, 2, 2, 8, 3, 6, 8, 2, 8, 5, 0, 3, 9, 7, 4, 3, 2, 2, 1,\n",
       "        0, 9, 8, 9, 5, 5, 3, 4, 7, 3, 2, 8, 1, 7, 3, 8])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl1.to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b9df3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2, lbl2 = next(iter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e86c24aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 1, 9, 3, 6, 9, 1, 5, 2, 4, 1, 1, 1, 6, 9, 2, 2, 5, 3, 7, 9, 7, 9,\n",
       "        3, 6, 4, 8, 6, 1, 1, 0, 4, 2, 6, 3, 4, 3, 7, 7, 8, 7, 2, 0, 7, 2, 9, 5,\n",
       "        2, 2, 4, 2, 1, 1, 6, 9, 5, 1, 3, 1, 6, 9, 5, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d5fb4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img3, lbl3 = next(iter3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c98a0bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 9, 0, 3, 9, 5, 0, 2, 0, 6, 5, 9, 3, 6, 3, 3, 1, 1, 4, 3, 6, 5, 7, 9,\n",
       "        2, 3, 1, 3, 8, 2, 7, 3, 2, 7, 9, 0, 3, 8, 4, 5, 9, 6, 1, 5, 6, 2, 1, 2,\n",
       "        8, 3, 7, 3, 0, 2, 8, 5, 0, 1, 9, 6, 3, 2, 8, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07c64c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.0676), tensor(1.), tensor(0.2293))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1[0].flatten().min(), img1[0].flatten().mean(), img1[0].flatten().max(), img1[0].flatten().std() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45e4a002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(img1[0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48fd2697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.1723), tensor(1.), tensor(0.3543))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2[0].flatten().min(), img2[0].flatten().mean(), img2[0].flatten().max(), img2[0].flatten().std() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51fdb3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(img2[0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6b083de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.1245), tensor(1.), tensor(0.3124))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img3[0].flatten().min(), img3[0].flatten().mean(), img3[0].flatten().max(), img3[0].flatten().std() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5ac6f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(img3[0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef6ebdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(0.0676) tensor(1.) tensor(0.2293)\n",
      "tensor(0.) tensor(0.1769) tensor(1.) tensor(0.3550)\n",
      "tensor(0.) tensor(0.1471) tensor(1.) tensor(0.3220)\n",
      "tensor(0.) tensor(0.1228) tensor(1.) tensor(0.2938)\n",
      "tensor(0.) tensor(0.1113) tensor(1.) tensor(0.2859)\n",
      "============================\n",
      "mean_mean_of_batch: 0.13264235854148865, mean_std_of_batch: 0.30367863178253174\n"
     ]
    }
   ],
   "source": [
    "mean_mean_of_batch = []\n",
    "mean_std_of_batch = []\n",
    "for i in range(img1.shape[0]):\n",
    "    mean_mean_of_batch.append(img1[i][0].mean())\n",
    "    mean_std_of_batch.append(img1[i][0].std())\n",
    "    if i < 5:\n",
    "        print(img1[i][0].min(), img1[i][0].mean(), img1[i][0].max(), img1[i][0].std())\n",
    "print(\"============================\")\n",
    "print(f\"mean_mean_of_batch: {np.array(mean_mean_of_batch).mean()}, mean_std_of_batch: {np.array(mean_std_of_batch).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2a0df38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(0.1723) tensor(1.) tensor(0.3543)\n",
      "tensor(0.) tensor(0.1632) tensor(1.) tensor(0.3396)\n",
      "tensor(0.) tensor(0.0899) tensor(1.) tensor(0.2708)\n",
      "tensor(0.) tensor(0.0998) tensor(1.) tensor(0.2672)\n",
      "tensor(0.) tensor(0.1518) tensor(1.) tensor(0.3325)\n",
      "============================\n",
      "mean_mean_of_batch: 0.12863576412200928, mean_std_of_batch: 0.2995503842830658\n"
     ]
    }
   ],
   "source": [
    "mean_mean_of_batch = []\n",
    "mean_std_of_batch = []\n",
    "for i in range(img2.shape[0]):\n",
    "    mean_mean_of_batch.append(img2[i][0].mean())\n",
    "    mean_std_of_batch.append(img2[i][0].std())\n",
    "    if i < 5:\n",
    "        print(img2[i][0].min(), img2[i][0].mean(), img2[i][0].max(), img2[i][0].std())\n",
    "print(\"============================\")\n",
    "print(f\"mean_mean_of_batch: {np.array(mean_mean_of_batch).mean()}, mean_std_of_batch: {np.array(mean_std_of_batch).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a50cf70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(0.1245) tensor(1.) tensor(0.3124)\n",
      "tensor(0.) tensor(0.1447) tensor(1.) tensor(0.3287)\n",
      "tensor(0.) tensor(0.1506) tensor(1.) tensor(0.3248)\n",
      "tensor(0.) tensor(0.1848) tensor(1.) tensor(0.3617)\n",
      "tensor(0.) tensor(0.0948) tensor(1.) tensor(0.2688)\n",
      "============================\n",
      "mean_mean_of_batch: 0.13368050754070282, mean_std_of_batch: 0.30511733889579773\n"
     ]
    }
   ],
   "source": [
    "mean_mean_of_batch = []\n",
    "mean_std_of_batch = []\n",
    "for i in range(img3.shape[0]):\n",
    "    mean_mean_of_batch.append(img3[i][0].mean())\n",
    "    mean_std_of_batch.append(img3[i][0].std())\n",
    "    if i < 5:\n",
    "        print(img3[i][0].min(), img3[i][0].mean(), img3[i][0].max(), img3[i][0].std())\n",
    "print(\"============================\")\n",
    "print(f\"mean_mean_of_batch: {np.array(mean_mean_of_batch).mean()}, mean_std_of_batch: {np.array(mean_std_of_batch).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb63061c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show 3 random images of data set\n",
    "for i in (np.random.rand(3)*100).astype('int'):\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.imshow(kaggle_train_loader.dataset.x_train[i].squeeze(), interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d0b55de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      "  (norm1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(20, 40, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(40, 60, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      "  (norm3): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=960, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 20, 2, padding=1)\n",
    "        self.norm1 = nn.BatchNorm2d(20)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(20, 40, 2, padding=1)\n",
    "        self.norm2 = nn.BatchNorm2d(40)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(40, 60, 2, padding=1)\n",
    "        self.norm3 = nn.BatchNorm2d(60)\n",
    "\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(60 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 10)\n",
    "        \n",
    "        # dropout layer (p=0.25)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = x.view(x.shape[0],1,28,28)\n",
    "        \n",
    "        x = self.norm1(self.pool(F.relu(self.conv1(x))))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.norm2(self.pool(F.relu(self.conv2(x))))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.norm3(self.pool(F.relu(self.conv3(x))))\n",
    "        \n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 60 * 4 * 4)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "# create a complete CNN\n",
    "model = Net().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ac4b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f896b774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle train Loss: 2.304414\n",
      "\n",
      "Kaggle train Accuracy of     0:  0% ( 0/284)\n",
      "Kaggle train Accuracy of     1:  0% ( 0/275)\n",
      "Kaggle train Accuracy of     2:  0% ( 0/254)\n",
      "Kaggle train Accuracy of     3:  0% ( 0/254)\n",
      "Kaggle train Accuracy of     4:  0% ( 0/242)\n",
      "Kaggle train Accuracy of     5: 100% (239/239)\n",
      "Kaggle train Accuracy of     6:  0% ( 0/258)\n",
      "Kaggle train Accuracy of     7:  0% ( 0/270)\n",
      "Kaggle train Accuracy of     8:  0% ( 0/219)\n",
      "Kaggle train Accuracy of     9:  0% ( 0/265)\n",
      "\n",
      "Kaggle train (Overall): 9% (239/2560)\n",
      "======================================\n",
      "Test Loss: 2.303120\n",
      "\n",
      "Train Accuracy of     0:  0% ( 0/376)\n",
      "Train Accuracy of     1:  0% ( 0/447)\n",
      "Train Accuracy of     2:  0% ( 0/365)\n",
      "Train Accuracy of     3:  0% ( 0/371)\n",
      "Train Accuracy of     4:  0% ( 0/377)\n",
      "Train Accuracy of     5: 100% (365/365)\n",
      "Train Accuracy of     6:  0% ( 0/388)\n",
      "Train Accuracy of     7:  0% ( 0/387)\n",
      "Train Accuracy of     8:  0% ( 0/378)\n",
      "Train Accuracy of     9:  0% ( 0/386)\n",
      "\n",
      "Train Accuracy (Overall): 9% (365/3840)\n",
      "=====================================\n",
      "Test Loss: 2.305053\n",
      "\n",
      "Test Accuracy of     0:  0% ( 0/117)\n",
      "Test Accuracy of     1:  0% ( 0/153)\n",
      "Test Accuracy of     2:  0% ( 0/141)\n",
      "Test Accuracy of     3:  0% ( 0/124)\n",
      "Test Accuracy of     4:  0% ( 0/128)\n",
      "Test Accuracy of     5: 100% (114/114)\n",
      "Test Accuracy of     6:  0% ( 0/137)\n",
      "Test Accuracy of     7:  0% ( 0/118)\n",
      "Test Accuracy of     8:  0% ( 0/117)\n",
      "Test Accuracy of     9:  0% ( 0/131)\n",
      "\n",
      "Test Accuracy (Overall): 8% (114/1280)\n"
     ]
    }
   ],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "valid_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval() # prep model for training\n",
    "\n",
    "for data, target in kaggle_train_valid_loader:\n",
    "    target = torch.tensor(np.array(target)).to(torch.int64)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data.to(device))\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target.to(device))\n",
    "    # update test loss \n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.to(device).data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(64):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "valid_loss = valid_loss/len(kaggle_train_valid_loader.sampler)\n",
    "print('Kaggle train Loss: {:.6f}\\n'.format(valid_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Kaggle train Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Kaggle train Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nKaggle train (Overall): %0d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n",
    "print('======================================')\n",
    "\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "\n",
    "# initialize lists to monitor test loss and accuracy\n",
    "valid_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval() # prep model for training\n",
    "\n",
    "for data, target in train_valid_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data.to(device))\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target.to(device))\n",
    "    # update test loss \n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.to(device).data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(64):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "valid_loss = valid_loss/len(train_valid_loader.sampler)\n",
    "print('Test Loss: {:.6f}\\n'.format(valid_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Train Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Train Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTrain Accuracy (Overall): %0d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n",
    "\n",
    "print('=====================================')\n",
    "\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "# initialize lists to monitor test loss and accuracy\n",
    "valid_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval() # prep model for training\n",
    "\n",
    "for data, target in test_valid_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data.to(device))\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target.to(device))\n",
    "    # update test loss \n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.to(device).data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(64):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "valid_loss = valid_loss/len(test_valid_loader.sampler)\n",
    "print('Test Loss: {:.6f}\\n'.format(valid_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %0d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "# batch_size is 70, so number of batches is 600\n",
    "# loop for 1 epoch\n",
    "epochs = 50\n",
    "accuracy_min = 0.997135\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(f\"Epoch number {e+1}\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    kaggle_train_loss = 0\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    \n",
    "    \n",
    "    for data, target in kaggle_train_loader:\n",
    "        target = torch.tensor(np.array(target)).to(torch.int64)\n",
    "        X_batch = data.to(device)\n",
    "        y_batch = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(X_batch), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        kaggle_train_loss += loss.item()\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        target = torch.tensor(np.array(target)).to(torch.int64)\n",
    "        X_batch = data.to(device)\n",
    "        y_batch = torch.tensor(np.array(target)).to(torch.int64).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(X_batch), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    for data, target in test_loader:\n",
    "        target = torch.tensor(np.array(target)).to(torch.int64)\n",
    "        X_batch = data.to(device)\n",
    "        y_batch = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(X_batch), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        test_loss += loss.item()\n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()    \n",
    "    for data, target in kaggle_train_valid_loader:\n",
    "        target = torch.tensor(np.array(target)).to(torch.int64)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data.to(device))\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.to(device))\n",
    "        # update test loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)\n",
    "        # compare predictions to true label\n",
    "        correct = np.squeeze(pred.eq(target.to(device).data.view_as(pred)))\n",
    "        # calculate test accuracy for each object class\n",
    "        for i in range(64):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "            \n",
    "    for data, target in train_valid_loader:\n",
    "        target = torch.tensor(np.array(target)).to(torch.int64)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data.to(device))\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.to(device))\n",
    "        # update test loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)\n",
    "        # compare predictions to true label\n",
    "        correct = np.squeeze(pred.eq(target.to(device).data.view_as(pred)))\n",
    "        # calculate test accuracy for each object class\n",
    "        for i in range(64):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "    \n",
    "    for data, target in test_valid_loader:\n",
    "        target = torch.tensor(np.array(target)).to(torch.int64)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data.to(device))\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.to(device))\n",
    "        # update test loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)\n",
    "        # compare predictions to true label\n",
    "        correct = np.squeeze(pred.eq(target.to(device).data.view_as(pred)))\n",
    "        # calculate test accuracy for each object class\n",
    "        for i in range(64):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "            \n",
    "    accuracy = np.sum(class_correct) / np.sum(class_total)\n",
    "    print(f\"train_loss: {kaggle_train_loss/len(kaggle_train_loader.sampler)}, train_loss: {train_loss/len(train_loader.sampler)}, train_loss: {test_loss/len(test_loader.sampler)}, accuracy on valid_set: {accuracy.item()}\")\n",
    "    if accuracy >= accuracy_min:\n",
    "        print('Accuracy metrics increased ({:.6f} --> {:.6f}).  Saving model ...'.format(accuracy_min,accuracy))\n",
    "        torch.save(model.state_dict(), 'MNIST-CNN.pt')\n",
    "        accuracy_min = accuracy\n",
    "    print(\"=========================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "071c77be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('MNIST-CNN.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34367513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle train Loss: 0.004343\n",
      "\n",
      "Kaggle train Accuracy of     0: 100% (284/284)\n",
      "Kaggle train Accuracy of     1: 100% (275/275)\n",
      "Kaggle train Accuracy of     2: 100% (254/254)\n",
      "Kaggle train Accuracy of     3: 100% (254/254)\n",
      "Kaggle train Accuracy of     4: 100% (242/242)\n",
      "Kaggle train Accuracy of     5: 99% (238/239)\n",
      "Kaggle train Accuracy of     6: 100% (258/258)\n",
      "Kaggle train Accuracy of     7: 100% (270/270)\n",
      "Kaggle train Accuracy of     8: 100% (219/219)\n",
      "Kaggle train Accuracy of     9: 99% (264/265)\n",
      "\n",
      "Kaggle train (Overall): 99% (2558/2560)\n",
      "======================================\n",
      "Test Loss: 0.005454\n",
      "\n",
      "Train Accuracy of     0: 100% (376/376)\n",
      "Train Accuracy of     1: 99% (446/447)\n",
      "Train Accuracy of     2: 99% (364/365)\n",
      "Train Accuracy of     3: 99% (369/371)\n",
      "Train Accuracy of     4: 100% (377/377)\n",
      "Train Accuracy of     5: 99% (364/365)\n",
      "Train Accuracy of     6: 99% (387/388)\n",
      "Train Accuracy of     7: 100% (387/387)\n",
      "Train Accuracy of     8: 99% (377/378)\n",
      "Train Accuracy of     9: 99% (384/386)\n",
      "\n",
      "Train Accuracy (Overall): 99% (3831/3840)\n",
      "=====================================\n",
      "Test Loss: 0.004384\n",
      "\n",
      "Test Accuracy of     0: 100% (117/117)\n",
      "Test Accuracy of     1: 100% (153/153)\n",
      "Test Accuracy of     2: 100% (141/141)\n",
      "Test Accuracy of     3: 100% (124/124)\n",
      "Test Accuracy of     4: 100% (128/128)\n",
      "Test Accuracy of     5: 100% (114/114)\n",
      "Test Accuracy of     6: 100% (137/137)\n",
      "Test Accuracy of     7: 100% (118/118)\n",
      "Test Accuracy of     8: 99% (116/117)\n",
      "Test Accuracy of     9: 99% (130/131)\n",
      "\n",
      "Test Accuracy (Overall): 99% (1278/1280)\n"
     ]
    }
   ],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "valid_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval() # prep model for training\n",
    "\n",
    "for data, target in kaggle_train_valid_loader:\n",
    "    target = torch.tensor(np.array(target)).to(torch.int64)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data.to(device))\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target.to(device))\n",
    "    # update test loss \n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.to(device).data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(64):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "valid_loss = valid_loss/len(kaggle_train_valid_loader.sampler)\n",
    "print('Kaggle train Loss: {:.6f}\\n'.format(valid_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Kaggle train Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Kaggle train Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nKaggle train (Overall): %0d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n",
    "print('======================================')\n",
    "\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "\n",
    "# initialize lists to monitor test loss and accuracy\n",
    "valid_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval() # prep model for training\n",
    "\n",
    "for data, target in train_valid_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data.to(device))\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target.to(device))\n",
    "    # update test loss \n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.to(device).data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(64):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "valid_loss = valid_loss/len(train_valid_loader.sampler)\n",
    "print('Test Loss: {:.6f}\\n'.format(valid_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Train Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Train Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTrain Accuracy (Overall): %0d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n",
    "\n",
    "print('=====================================')\n",
    "\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "# initialize lists to monitor test loss and accuracy\n",
    "valid_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval() # prep model for training\n",
    "\n",
    "for data, target in test_valid_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data.to(device))\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target.to(device))\n",
    "    # update test loss \n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.to(device).data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(64):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "valid_loss = valid_loss/len(test_valid_loader.sampler)\n",
    "print('Test Loss: {:.6f}\\n'.format(valid_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %0d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "234fa885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classify(img, ps, version=\"MNIST\"):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "#     if version == \"MNIST\":\n",
    "#         ax2.set_yticklabels(np.arange(10))\n",
    "#     elif version == \"Fashion\":\n",
    "#         ax2.set_yticklabels(['T-shirt/top',\n",
    "#                             'Trouser',\n",
    "#                             'Pullover',\n",
    "#                             'Dress',\n",
    "#                             'Coat',\n",
    "#                             'Sandal',\n",
    "#                             'Shirt',\n",
    "#                             'Sneaker',\n",
    "#                             'Bag',\n",
    "#                             'Ankle Boot'], size='small');\n",
    "#     ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "98b2c2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit = 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter1)\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img.cuda())\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps).cpu()\n",
    "probab = list(ps.cpu().numpy()[0])\n",
    "print(\"Predicted Digit =\", probab.index(max(probab)))\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dcddd051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28000, 784)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('C:/datasets/digit-recognizer/test.csv').values; submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e385ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = scaler.transform(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddbb756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = torch.tensor(submission, dtype=torch.float32).view(28000,1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df89c6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28000, 1, 28, 28])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7fcb00ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      output\n",
       "0          2\n",
       "1          0\n",
       "2          9\n",
       "3          0\n",
       "4          3\n",
       "...      ...\n",
       "6995       2\n",
       "6996       0\n",
       "6997       4\n",
       "6998       4\n",
       "6999       3\n",
       "\n",
       "[7000 rows x 1 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values1, preds1 = torch.topk(model(submission[:7000].to(device)), 1)\n",
    "# preds1 = pd.DataFrame(preds1.cpu()).rename(columns={0:'output'})\n",
    "# preds1.to_csv('C:/datasets/digit-recognizer/outputs/preds1.csv'); preds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "613ceeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      output\n",
       "0          8\n",
       "1          1\n",
       "2          7\n",
       "3          6\n",
       "4          5\n",
       "...      ...\n",
       "6995       9\n",
       "6996       3\n",
       "6997       5\n",
       "6998       4\n",
       "6999       0\n",
       "\n",
       "[7000 rows x 1 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values2, preds2 = torch.topk(model(submission[7000:14000].to(device)), 1)\n",
    "# preds2 = pd.DataFrame(preds2.cpu()).rename(columns={0:'output'})\n",
    "# preds2.to_csv('C:/datasets/digit-recognizer/outputs/preds2.csv'); preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0ad9b049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      output\n",
       "0          3\n",
       "1          4\n",
       "2          3\n",
       "3          6\n",
       "4          7\n",
       "...      ...\n",
       "6995       6\n",
       "6996       1\n",
       "6997       4\n",
       "6998       2\n",
       "6999       6\n",
       "\n",
       "[7000 rows x 1 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values3, preds3 = torch.topk(model(submission[14000:21000].to(device)), 1)\n",
    "# preds3 = pd.DataFrame(preds3.cpu()).rename(columns={0:'output'})\n",
    "# preds3.to_csv('C:/datasets/digit-recognizer/outputs/preds3.csv'); preds3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "057583ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      output\n",
       "0          7\n",
       "1          5\n",
       "2          5\n",
       "3          4\n",
       "4          2\n",
       "...      ...\n",
       "6995       9\n",
       "6996       7\n",
       "6997       3\n",
       "6998       9\n",
       "6999       2\n",
       "\n",
       "[7000 rows x 1 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values4, preds4 = torch.topk(model(submission[21000:28000].to(device)), 1)\n",
    "# preds4 = pd.DataFrame(preds4.cpu()).rename(columns={0:'output'})\n",
    "# preds4.to_csv('C:/datasets/digit-recognizer/outputs/preds4.csv'); preds4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ef8f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds1 = pd.read_csv('C:/datasets/digit-recognizer/outputs/preds1.csv').drop('Unnamed: 0',axis=1); \n",
    "preds2 = pd.read_csv('C:/datasets/digit-recognizer/outputs/preds2.csv').drop('Unnamed: 0',axis=1); \n",
    "preds3 = pd.read_csv('C:/datasets/digit-recognizer/outputs/preds3.csv').drop('Unnamed: 0',axis=1); \n",
    "preds4 = pd.read_csv('C:/datasets/digit-recognizer/outputs/preds4.csv').drop('Unnamed: 0',axis=1); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c5e04921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2\n",
       "1       0\n",
       "2       9\n",
       "3       0\n",
       "4       3\n",
       "       ..\n",
       "6995    9\n",
       "6996    3\n",
       "6997    5\n",
       "6998    4\n",
       "6999    0\n",
       "Name: output, Length: 14000, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([preds1['output'], preds2['output']], axis=0); df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92e8ceaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       3\n",
       "1       4\n",
       "2       3\n",
       "3       6\n",
       "4       7\n",
       "       ..\n",
       "6995    9\n",
       "6996    7\n",
       "6997    3\n",
       "6998    9\n",
       "6999    2\n",
       "Name: output, Length: 14000, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([preds3['output'], preds4['output']], axis=0); df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf2748",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2], axis=0); df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bcca76b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       output\n",
       "0           2\n",
       "1           0\n",
       "2           9\n",
       "3           0\n",
       "4           3\n",
       "...       ...\n",
       "27995       9\n",
       "27996       7\n",
       "27997       3\n",
       "27998       9\n",
       "27999       2\n",
       "\n",
       "[28000 rows x 1 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index().drop('index', axis=1); df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea19ff61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      0\n",
       "1            2      0\n",
       "2            3      0\n",
       "3            4      0\n",
       "4            5      0\n",
       "...        ...    ...\n",
       "27995    27996      0\n",
       "27996    27997      0\n",
       "27997    27998      0\n",
       "27998    27999      0\n",
       "27999    28000      0\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv('C:/datasets/digit-recognizer/sample_submission.csv'); sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "07c7235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['Label'] = df['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "311fae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('C:/datasets/digit-recognizer/sub_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8ff72f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      9\n",
       "3            4      0\n",
       "4            5      3\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('C:/datasets/digit-recognizer/sub_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7a31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58af4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878cf8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1a782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
